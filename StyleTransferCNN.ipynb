{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "StyleTransferCNN.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "JLgvm2ecvyjy"
      },
      "source": [
        "import time\n",
        "import os \n",
        "import torch\n",
        "from torch.autograd import Variable\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch import optim\n",
        "import torchvision\n",
        "from torchvision import transforms\n",
        "from PIL import Image\n",
        "from collections import OrderedDict\n",
        "import matplotlib.pyplot as plt"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3y1FNeQov8SU"
      },
      "source": [
        "class VGG(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(VGG, self).__init__()\n",
        "        self.conv1_1 = nn.Conv2d(3, 64, kernel_size = 3, padding = 1)\n",
        "        self.conv1_2 = nn.Conv2d(64, 64, kernel_size = 3, padding = 1)\n",
        "        \n",
        "        self.conv2_1 = nn.Conv2d(64, 128, kernel_size = 3, padding = 1)\n",
        "        self.conv2_2 = nn.Conv2d(128, 128, kernel_size = 3, padding = 1)\n",
        "        \n",
        "        self.conv3_1 = nn.Conv2d(128, 256, kernel_size = 3, padding = 1)\n",
        "        self.conv3_2 = nn.Conv2d(256, 256, kernel_size = 3, padding = 1)\n",
        "        self.conv3_3 = nn.Conv2d(256, 256, kernel_size = 3, padding = 1)\n",
        "        self.conv3_4 = nn.Conv2d(256, 256, kernel_size = 3, padding = 1)\n",
        "        \n",
        "        self.conv4_1 = nn.Conv2d(256, 512, kernel_size = 3, padding = 1)\n",
        "        self.conv4_2 = nn.Conv2d(512, 512, kernel_size = 3, padding = 1)\n",
        "        self.conv4_3 = nn.Conv2d(512, 512, kernel_size = 3, padding = 1)\n",
        "        self.conv4_4 = nn.Conv2d(512, 512, kernel_size = 3, padding = 1)\n",
        "        \n",
        "        self.conv5_1 = nn.Conv2d(512, 512, kernel_size = 3, padding = 1)\n",
        "        self.conv5_2 = nn.Conv2d(512, 512, kernel_size = 3, padding = 1)\n",
        "        self.conv5_3 = nn.Conv2d(512, 512, kernel_size = 3, padding = 1)\n",
        "        self.conv5_4 = nn.Conv2d(512, 512, kernel_size = 3, padding = 1)\n",
        "        \n",
        "        self.pool1 = nn.AvgPool2d(kernel_size = 2, stride = 2)\n",
        "        self.pool2 = nn.AvgPool2d(kernel_size = 2, stride = 2)\n",
        "        self.pool3 = nn.AvgPool2d(kernel_size = 2, stride = 2)\n",
        "        self.pool4 = nn.AvgPool2d(kernel_size = 2, stride = 2)\n",
        "        self.pool5 = nn.AvgPool2d(kernel_size = 2, stride = 2)\n",
        "            \n",
        "\n",
        "    def forward(self, x, out_keys):\n",
        "        out = {}\n",
        "        \n",
        "        out['r11'] = F.relu(self.conv1_1(x))\n",
        "        out['r12'] = F.relu(self.conv1_2(out['r11']))\n",
        "        out['p1'] = self.pool1(out['r12'])\n",
        "        \n",
        "        out['r21'] = F.relu(self.conv2_1(out['p1']))\n",
        "        out['r22'] = F.relu(self.conv2_2(out['r21']))\n",
        "        out['p2'] = self.pool2(out['r22'])\n",
        "        \n",
        "        out['r31'] = F.relu(self.conv3_1(out['p2']))\n",
        "        out['r32'] = F.relu(self.conv3_2(out['r31']))\n",
        "        out['r33'] = F.relu(self.conv3_3(out['r32']))\n",
        "        out['r34'] = F.relu(self.conv3_4(out['r33']))\n",
        "        out['p3'] = self.pool3(out['r34'])\n",
        "        \n",
        "        out['r41'] = F.relu(self.conv4_1(out['p3']))\n",
        "        out['r42'] = F.relu(self.conv4_2(out['r41']))\n",
        "        out['r43'] = F.relu(self.conv4_3(out['r42']))\n",
        "        out['r44'] = F.relu(self.conv4_4(out['r43']))\n",
        "        out['p4'] = self.pool4(out['r44'])\n",
        "        \n",
        "        out['r51'] = F.relu(self.conv5_1(out['p4']))\n",
        "        out['r52'] = F.relu(self.conv5_2(out['r51']))\n",
        "        out['r53'] = F.relu(self.conv5_3(out['r52']))\n",
        "        out['r54'] = F.relu(self.conv5_4(out['r53']))\n",
        "        out['p5'] = self.pool5(out['r54'])\n",
        "        \n",
        "        \n",
        "        return [out[key] for key in out_keys]"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ScQrixzUxEB9"
      },
      "source": [
        "class GramMatrix(nn.Module):\n",
        "    def forward(self, input):\n",
        "        batchnum, channelnum, width, height = input.size()\n",
        "        F = input.view(batchnum, channelnum, height * width)\n",
        "        return torch.bmm(F, F.transpose(1, 2)).div(height*width)\n",
        "\n",
        "\n",
        "class GramMSELoss(nn.Module):\n",
        "    def forward(self, input, target):\n",
        "        Gram = GramMatrix()\n",
        "        out = nn.MSELoss()(Gram(input), target)\n",
        "        return out\n",
        "\n",
        "img_size = 512\n",
        "prep = transforms.Compose([transforms.Resize(img_size),\n",
        "                           transforms.ToTensor(),\n",
        "                           transforms.Lambda(lambda x: x[torch.LongTensor([2, 1, 0])]),\n",
        "                           transforms.Normalize(mean = [0.40760392, 0.45795686, 0.48501961], std = [1, 1, 1]),\n",
        "                           transforms.Lambda(lambda x: x.mul(255)), ])\n",
        "postpa = transforms.Compose([transforms.Lambda(lambda x: x.mul(1./255)),\n",
        "                            transforms.Normalize(mean = [-0.40760392, -0.45795686, -0.48501961], std = [1, 1, 1]),\n",
        "                            transforms.Lambda(lambda x: x[torch.LongTensor([2,1,0])]), ])\n",
        "postpb = transforms.Compose([transforms.ToPILImage()])\n",
        "def postp(tensor):\n",
        "    t = postpa(tensor)\n",
        "    t[t>1], t[t<0] = 1, 0\n",
        "    img = postpb(t)\n",
        "    return img"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4O50tiC9asII"
      },
      "source": [
        "vgg = VGG()\n",
        "vgg.load_state_dict(torch.load(\"vgg_conv_weights.pth\"))            \n",
        "for param in vgg.parameters():\n",
        "    param.requires_grad = False\n",
        "if torch.cuda.is_available():\n",
        "    vgg.cuda()\n",
        "img_names = ['style_monet_sunset.jpg', 'IMG_0995.JPG']\n",
        "imgs = [Image.open(\"/content/gdrive/My Drive/cs194/StyleTransfer/Images/\" + name) for name in img_names]\n",
        "imgs_torch = [prep(img) for img in imgs]\n",
        "if torch.cuda.is_available():\n",
        "    imgs_torch = [Variable(img.unsqueeze(0)).cuda() for img in imgs_torch]\n",
        "else:\n",
        "    imgs_torch = [Variable(img.unsqueeze(0)) for img in imgs_torch]\n",
        "style_img, content_img = imgs_torch\n",
        "opt_img = Variable(content_img.clone(), requires_grad = True)\n",
        "style_layers = ['r11', 'r12', 'r31', 'r41', 'r51']\n",
        "content_layers = ['r42']\n",
        "loss_layers = style_layers + content_layers\n",
        "loss_fns = [GramMSELoss()] * len(style_layers) + [nn.MSELoss()] * len(content_layers)\n",
        "if torch.cuda.is_available():\n",
        "    loss_fns = [loss_fn.cuda() for loss_fn in loss_fns] \n",
        "style_weights = [1e3/n**2 for n in [64, 128, 256, 512, 512]]\n",
        "content_weights = [1e0]\n",
        "weights = style_weights + content_weights\n",
        "style_targets = [GramMatrix()(A).detach() for A in vgg(style_img, style_layers)]\n",
        "content_targets = [A.detach() for A in vgg(content_img, content_layers)]\n",
        "targets = style_targets + content_targets"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_5NrT_16FLaY",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "bef80bf2-4344-4b90-ac7c-8d4fb80e8357"
      },
      "source": [
        "max_iter = 500\n",
        "show_iter = 30\n",
        "optimizer = optim.LBFGS([opt_img])\n",
        "print(opt_img.size())\n",
        "print(content_img.size())\n",
        "n_iter = [0]\n",
        "while n_iter[0] <= max_iter:\n",
        "  \n",
        "    def closure():\n",
        "        optimizer.zero_grad()\n",
        "        out = vgg(opt_img, loss_layers)\n",
        "        layer_losses = [weights[a] * loss_fns[a](A, targets[a]) for a,A in enumerate(out)]\n",
        "        loss = sum(layer_losses)\n",
        "        loss.backward()\n",
        "        n_iter[0] += 1\n",
        "        if n_iter[0] % show_iter == (show_iter - 1):\n",
        "            print('Iteration: %d,\\tLoss: %f' % (n_iter[0] + 1, loss.item()))\n",
        "            \n",
        "        return loss\n",
        "    \n",
        "    optimizer.step(closure)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "torch.Size([1, 3, 512, 682])\n",
            "torch.Size([1, 3, 512, 682])\n",
            "Iteration: 30,\tLoss: 357097.281250\n",
            "Iteration: 60,\tLoss: 66715.140625\n",
            "Iteration: 90,\tLoss: 50542.398438\n",
            "Iteration: 120,\tLoss: 45328.500000\n",
            "Iteration: 150,\tLoss: 42671.773438\n",
            "Iteration: 180,\tLoss: 40939.046875\n",
            "Iteration: 210,\tLoss: 39777.316406\n",
            "Iteration: 240,\tLoss: 38941.500000\n",
            "Iteration: 270,\tLoss: 38325.320312\n",
            "Iteration: 300,\tLoss: 37881.812500\n",
            "Iteration: 330,\tLoss: 37527.433594\n",
            "Iteration: 360,\tLoss: 37239.613281\n",
            "Iteration: 390,\tLoss: 37009.386719\n",
            "Iteration: 420,\tLoss: 36818.699219\n",
            "Iteration: 450,\tLoss: 36657.097656\n",
            "Iteration: 480,\tLoss: 36518.562500\n",
            "Iteration: 510,\tLoss: 36406.750000\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "i7prhhKAawXv"
      },
      "source": [
        "out_img = postp(opt_img.data[0].cpu().squeeze())\n",
        "# plt.grid(None)\n",
        "# plt.imshow(out_img)\n",
        "# plt.gcf().set_size_inches(10, 10)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J52I21yMPUtu"
      },
      "source": [
        "import skimage.io as skio\n",
        "import numpy as np\n",
        "skio.imsave(\"a.jpg\", np.asarray(out_img))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0h0CrREt-nuC"
      },
      "source": [
        "skio.imsave(\"/content/gdrive/My Drive/cs194/StyleTransfer/Images/claire2.JPG\", np.asarray(out_img))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "D1tEADZ-_kYD"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}